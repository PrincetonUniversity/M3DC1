1. login 
	ssh your_id@traverse.princeton.edu

2. code

3. load modules
   For nvhpc/21.5 and openmpi/4.0.3, you don't need to load any module

   For nvhpc/22.5, cuda/11.7 and openmpi/4.1.4,
      module load nvhpc/22.5 openmpi/cuda-11.7/nvhpc-22.5/4.1.4/64
 
4. compile code
   For nvhpc/21.5 and openmpi/4.0.3
	2D real: make OPT=1 ARCH=traverse-nvhpc
	2D complex: make OPT=1 COM=1 ARCH=traverse-nvhpc
            - add PAR=1 to run PIC
	3D real: make 3D=1 OPT=1 MAX_PTS=60 ARCH=traverse-nvhpc
        
   For nvhpc/22.5, cuda/11.7 and openmpi/4.1.4
        2D real: make OPT=1 ARCH=traverse-nvhpc-cuda
        2D complex: make OPT=1 COM=1 ARCH=traverse-nvhpc-cuda
            - add PAR=1 to run PIC
        3D real: make 3D=1 OPT=1 MAX_PTS=60 ARCH=traverse-nvhpc-cuda
    
5. mesh utility
   See $SCOREC_UTIL_DIR in ARCH.mk file. 

   For nvhpc/21.5 and openmpi/4.0.3
        /projects/M3DC1/scorec/traverse/nvhpc21.5-openmpi4.0.3/petsc-3.18.3/bin

   For nvhpc/22.5, cuda/11.7 and openmpi/4.1.4
        /projects/M3DC1/scorec/traverse/nvhpc22.5-openmpi4.1.4/petsc3.18.3/bin

6. run a job 

6.1 interactive mode
  
   salloc -n N -t 00:30:00 -p pppl

6.2 batch mode

  sample job_script
        #2D complex
            #!/bin/bash
            #SBATCH -A pppl
            #SBATCH --reservation=test
            #SBATCH --nodes=2
            #SBATCH --ntasks-per-node=4
            #SBATCH --ntasks-per-socket=2
            #SBATCH --cpus-per-task=16
            #SBATCH --gpu-bind=map_gpu:0,1,2,3
            #SBATCH --gpus-per-task=1
            #SBATCH --time=00:59:00

	    srun -n 8 ./m3dc1_2d_complex -pc_factor_mat_solver_type mumps >& log.m3dc1_2Dcplx_mumps_gpu


        #2D real with petsc
            #!/bin/bash
            #SBATCH -A pppl
            #SBATCH --nodes=2
            #SBATCH --ntasks-per-node=4
            #SBATCH --ntasks-per-socket=2
            #SBATCH --cpus-per-task=16
            #SBATCH --gpu-bind=map_gpu:0,1,2,3
            #SBATCH --gpus-per-task=1
            #SBATCH -J CMOD04L_2DREAL
            #SBATCH --time=04:50:00

            export OMP_NUM_THREADS 1
            export OMP_PLACES=cores
            srun -n 8 ./m3dc1_2d -xmat_type aijcusparse -xvec_type cuda  >& log.m3dc1_2Dreal_superlu

        #3D real with petsc
            #!/bin/bash
            #SBATCH -A pppl
            #SBATCH --reservation=test
            #SBATCH --nodes=4
            #SBATCH --ntasks-per-node=8
            #SBATCH --ntasks-per-socket=4
            #SBATCH --cpus-per-task=16
            #SBATCH --gpu-bind=map_gpu:0,1,2,3
            #SBATCH --gpus=16
            #SBATCH --time=00:59:00

            srun -n 32 ./m3dc1_3d -ipetsc -options_file options_bjacobi.superlu >& log.m3dc1_3d


  submit job
	sbatch job_script

  list all curent jobs for a user
	squeue -u your_id

  delete a job
   	scancel job_id
