1. login 
	ssh your_id@eddy.princeton.edu

2. code
   git clone https://github.com/PrincetonUniversity/M3DC1.git

3. load modules
      Run code with OpenMPI:
      module load intel/18.0/64/18.0.3.222
      module load openmpi/intel-18.0/3.0.0/64
      module load gsl/2.4

      Run code with Intel-MPI:
      module load intel/18.0/64/18.0.3.222
      module load intel-mpi/intel/2018.3/64
      module load fftw/intel-16.0/intel-mpi/3.3.4
      module load hdf5/intel-16.0/intel-mpi/1.8.16
      module load gsl

4. compile code
	2D real: make OPT=1 RL=1 MAX_PTS=25 ARCH=eddy
	2D complex: make OPT=1 COM=1 MAX_PTS=25 ARCH=eddy
            - add PAR=1 to run PIC
	3D real: make 3D=1 OPT=1 MAX_PTS=60 ARCH=eddy
        
5. mesh utility
      Intel-MPI: /home/jinchen/LIB/scorec/intel18.0-mpi2018.3.64/petsc3.12.0/bin
      OpenMPI:   /home/jinchen/LIB/scorec/intel18.0-openmpi3.3.0/petsc3.12.0/bin

6. run a job 

6.1 interactive mode
  
  $salloc -n N -t HH:MM:SS 
  $module load intel/18.0/64/18.0.3.222 openmpi/intel-18.0/3.0.0/64 gsl/2.4
  (complex) $srun -n N ./m3dc1_2d_complex -pc_factor_mat_solver_package mumps
  (2D real) $srun -n N  ./m3dc1_2d
  (3D real) $srun -n N ./m3dc1_3d -ipetsc -options_file options_bjacobi
  $exit

6.2 batch mode

  sample job_script

	#!/bin/bash
	#SBATCH -A pppl
	#SBATCH --nodes=16
	#SBATCH --ntasks-per-node=16
	#SBATCH -J JOB
	#SBATCH --time=00:30:00
        module load intel/18.0/64/18.0.3.222 openmpi/intel-18.0/3.0.0/64 gsl/2.4
        #2D complex
            srun -n 8 ./m3dc1_2d_complex -pc_factor_mat_solver_package mumps
        #2D real with petsc
            srun -n 8 ./m3dc1_2d
        #3D real with petsc
            srun -n 256 ./m3dc1_3d -ipetsc -options_file options_bjacobi


  submit job
	sbatch job_script

  list all curent jobs for a user
	squeue -u your_id

  delete a job
   	scancel job_id
