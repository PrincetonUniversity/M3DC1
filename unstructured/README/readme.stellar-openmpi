******************************************************************************
 NOTE: please add the following runtime options at your srun line:

 -sub_mat_superlu_dist_rowperm norowperm (when using superlu_dist for 3D run)
 -sub_mat_mumps_icntl_14 100 (when using mumps for 3D run)
******************************************************************************

1. login 
	ssh your_id@stellar.princeton.edu

	Nodes have quad sockets with 24 cores/socket  per node.  There is
	8GB/core available so a MUCH larger memory footprint.  The back end
	network is 100Gb Infiniband, HDR100.

2. code
   git clone https://your_id:password@github.com/PrincetonUniversity/M3DC1.git

3. load modules
 
   module load intel/2021.1.2 openmpi/intel-2021.1/4.1.0
   module load fftw/intel-2021.1/openmpi-4.1.0/3.3.9
   module load hdf5/intel-2021.1/openmpi-4.1.0/1.10.6

4. compile code

   2D real: make OPT=1 RL=1 MAX_PTS=25 ARCH=stellar
   2D complex: make OPT=1 COM=1 MAX_PTS=25 ARCH=stellar
           - add PAR=1 to run PIC
   3D real: make 3D=1 OPT=1 MAX_PTS=60 ARCH=stellar
   after compiling, run "make bin"

5. mesh utility

5.1 mesh generation and conversion with Simmetrix
    module load rlm/pppl simmodsuite/pppl/17.0-220903
    /projects/M3DC1/scorec/intel2021.1.2-openmpi4.1.0/17.0-220903/bin

5.2 partition and merge
   /projects/M3DC1/scorec/intel2021.1.2-openmpi4.1.0/petsc3.13.5/bin/

6. run a job 

6.1 interactive mode
  
  $salloc -n N -t HH:MM:SS 
  $module load intel/2021.1.2 openmpi/intel-2021.1/4.1.0
  (complex) $srun -n N ./m3dc1_2d_complex -pc_factor_mat_solver_package mumps
  (2D real) $srun -n N  ./m3dc1_2d
  (3D real) $srun -n N ./m3dc1_3d -ipetsc -options_file options_bjacobi
  $exit

6.2 batch mode

  sample job_script

	#!/bin/bash
	#SBATCH -A pppl
	#SBATCH --nodes=16
	#SBATCH --ntasks-per-node=16
	#SBATCH -J JOB
	#SBATCH --time=00:30:00
        #2D complex
            srun -n 8 ./m3dc1_2d_complex -pc_factor_mat_solver_package mumps
        #2D real with petsc
            srun -n 8 ./m3dc1_2d
        #3D real with petsc
            srun -n 256 ./m3dc1_3d -ipetsc -options_file options_bjacobi


  submit job
	sbatch job_script

  list all curent jobs for a user
	squeue -u your_id

  delete a job
   	scancel job_id

6.3 regression test

	setenv M3DC1_MPIRUN srun
	setenv M3DC1_VERSION local
	setenv M3DC1_ARCH stellar-openmpi
	make bin ARCH=$M3DC1_ARCH
	cd _traverse/bin/
	setenv PATH `pwd`:$PATH
	cd ../../regtest/
	./run $M3DC1_ARCH
	./check $M3DC1_ARCH
