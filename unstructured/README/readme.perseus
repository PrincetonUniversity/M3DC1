1. login 
	ssh your_id@perseus.princeton.edu

2. code

3. load modules

      module load intel/18.0/64/18.0.3.222
      module load intel-mpi/intel/2018.3/64
      module load gsl/2.4
      module load hdf5/intel-17.0/intel-mpi/1.10.0
      module load fftw/intel-16.0/intel-mpi/3.3.4

4. compile code
	2D real: make OPT=1 
	2D complex: make OPT=1 COM=1 
            - add PAR=1 to run PIC
	3D real: make 3D=1 OPT=1 MAX_PTS=60 
        
5. mesh utility
        ??? /home/jinchen/LIB/intel17.0-openmpi1.10.2/bin

6. run a job 
    for details, see http://www.princeton.edu/researchcomputing/computational-hardware/perseus/user-guidelines

6.1 interactive mode
    # num_proc = N*T
    salloc --nodes=N --ntasks-per-node=T -t hh:mm:ss
    module load intel/17.0/64/17.0.5.239 openmpi gsl/2.4
    srun m3dc1_3d -ipetsc -options_file options_bjacobi

6.2 batch mode

  sample job_script

	#!/bin/bash
	# parallel 3D job using 32 processors and runs for 30 minutes (max: 4 hours) 
  	# max 64 nodes & 28 cores/node 
	#SBATCH --nodes=16 # node count
	#SBATCH --ntasks-per-node=28 
	#SBATCH --time=12:00:00 
	# sends mail when process begins, and
	# when it ends. Make sure you define your email 
	#SBATCH --mail-type=begin 
	#SBATCH --mail-type=end 
	#SBATCH --mail-user=your_email
	# Load mpi environment 
            module load intel/18.0/64/18.0.1.163
            module load intel-mpi/intel/2018.1/64
            module load gsl/2.4
            module load hdf5/intel-17.0/intel-mpi/1.10.0
            module load fftw/intel-16.0/intel-mpi/3.3.4
        #2D complex
            srun -n 28 m3dc1_2d_complex -pc_factor_mat_solver_package mumps
        #2D real with petsc
            srun -n 28 m3dc1_2d
        #3D real with petsc
            srun -n 448 m3dc1_3d -ipetsc -options_file options_bjacobi

  submit job
	sbatch job_script

  list all curent jobs for a user
	squeue -u your_id

  delete a job
   	scancel job_id
