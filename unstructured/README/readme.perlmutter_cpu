1. login: 

   ssh -P perlmutter-p1.nersc.gov -l [username]
   or
   ssh -P saul-p1.nersc.gov -l [username]

2. code
   git clone git@github.com:PrincetonUniversity/M3DC1.git 
   git clone -b [branch] git@github.com:PrincetonUniversity/M3DC1.git 
   git commit -m "perlmutter code porting"
   git push origin master

3. load modules and set environment
   module load cmake python/3.9-anaconda-2021.11
   module load PrgEnv-nvidia/8.3.3 fast-mkl-amd

   for development modules:
       export M3DC1_CODE_DIR=$HOME/src/M3DC1  [for example]
       module use $M3DC1_CODE_DIR/unstructured/modules/perlmutter
       module load m3dc1/devel-cpu

4. compile code

   make ARCH=perlmutter_cpu all
    (make OPT=1 RL=1 MAX_PTS=25 ARCH=perlmutter_cpu
   make OPT=1 COM=1 MAX_PTS=25 ARCH=perlmutter_cpu
   make 3D=1 OPT=1 MAX_PTS=60 ARCH=perlmutter_cpu
   make 3D=1 OPT=1 MAX_PTS=60 ARCH=perlmutter_cpu ST=1
    (These modules crashes at restart: cray-hdf5-parallel/1.12.0.7 cray-netcdf-hdf5parallel/4.7.4.7
     So rebuilt hdf5, netcdf-c, netcdf-fortran on 2/15/2022.)
    )

5. mesh utility
   see $SCOREC_UTIL_DIR in perlmutter_cpu.mk

6. options_bjacobi for 3D

        -pc_type bjacobi
        -pc_bjacobi_blocks 4
        -sub_pc_type lu
        -sub_pc_factor_mat_solver_type mumps
        -sub_mat_mumps_icntl_14 100
        -sub_ksp_type preonly
        -ksp_type fgmres
        -ksp_gmres_restart 220
        -ksp_rtol 1.e-9
        -ksp_atol 1.e-20
        -ksp_converged_reason

        -hard_pc_type bjacobi
        -hard_pc_bjacobi_blocks 4
        -hard_sub_pc_type lu
        -hard_sub_pc_factor_mat_solver_type mumps
        -hard_sub_mat_mumps_icntl_14 100
        -hard_sub_ksp_type preonly
        -hard_ksp_type fgmres
        -hard_ksp_gmres_restart 220
        -hard_ksp_rtol 1.e-9
        -hard_ksp_atol 1.e-20
        -hard_ksp_converged_reason

        -on_error_abort

   Please change the number of blocks (here 4) to match the number of plane in your 'C1input' file.

   We have another set of options with 'hard_" as the prefix for PETSC VERSION 3.8 or newer
   to isolate the hard solves (#5, #17) from easier ones for optimization purpose.

7. run job

7.1 interactive mode
  salloc -A mp288 -C cpu -n 48 -t 00:30:00
  export SLURM_CPU_BIND="cores"
  srun ... (see 7.2)

7.2 batch mode
   sample slurm script "batchjob.perlmutter_cpu"

   #!/bin/bash
   #SBATCH -A mp288 
   #SBATCH -q debug
   #SBATCH -C cpu
   #SBATCH -n 48
   #SBATCH -J m3dc1_regtest_adapt
   #SBATCH -t 0:30:00
   #SBATCH -o C1stdout

   export SLURM_CPU_BIND="cores"

   srun -n 32 -c 8 m3dc1_2d_complex -pc_factor_mat_solver_type mumps
   srun -n 32 -c 8 m3dc1_2d -pc_factor_mat_solver_type mumps
   srun -n 32 -c 8 m3dc1_3d -options_file options_bjacobi.type_mumps

8. regression tests
   cd unstructured
   export M3DC1_MPIRUN=srun M3DC1_VERSION=local M3DC1_ARCH=perlmutter_cpu
   make bin ARCH=$M3DC1_ARCH
   cd _perlmutter_cpu/bin/; export PATH=`pwd`:$PATH
   cd ../../regtest/
   ./run $M3DC1_ARCH
   ./check $M3DC1_ARCH

   run individual test
   ./run $M3DC1_ARCH pellet
   ./run $M3DC1_ARCH KPRAD_restart
   ./run $M3DC1_ARCH NCSX


