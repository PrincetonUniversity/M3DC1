1. login: 

   ssh -P eofe4.mit.edu -l [username]
   ssh -P eofe10.mit.edu -l [username]
   salloc -p sched_mit_psfc_r8 --nodes 1 --ntasks-per-node=2 --x11 -t 02:00:00

2. code
   git clone git@github.com:PrincetonUniversity/M3DC1.git 
   git clone -b [branch] git@github.com:PrincetonUniversity/M3DC1.git 
   git commit -m "mit code porting"
   git push origin master

3. load modules and set environment

   for development modules:
       export M3DC1_CODE_DIR=$HOME/src/M3DC1  [for example]
       module use $M3DC1_CODE_DIR/unstructured/modules/mit
       module load m3dc1/devel-intel

   or
   module use /orcd/nese/psfc/001/software/spack/2023-07-01-physics-rpp/spack/share/spack/modules-test/linux-rocky8-x86_64
   module load intel-oneapi-mkl/2023.1.0-intel-oneapi-mpi-2021.9.0-gcc-12.2.0-module-seow5nc

4. compile code

   make ARCH=mit all
    (make OPT=1 RL=1 MAX_PTS=25 ARCH=mit
     make OPT=1 COM=1 MAX_PTS=25 ARCH=mit
     make 3D=1 OPT=1 MAX_PTS=60 ARCH=mit
     make 3D=1 OPT=1 MAX_PTS=60 ARCH=mit
    )

5. mesh utility

5.1 mesh generation
  ssh eofe7.mit.edu
  module load gcc/6.2.0 openmpi/4.0.4
  executables are in /orcd/nese/psfc/001/software/scorec/rhel7-gcc6.2.0-openmpi4.0.4/sim18.0-230521/bin

  For more details, see readme and M3DC1 User's Guide (https://m3dc1.pppl.gov/M3DC1.pdf)

5.2 mesh partitioning and merging
  see $SCOREC_UTIL_DIR in mit.mk

6. options_bjacobi for 3D

        -pc_type bjacobi
        -pc_bjacobi_blocks 4
        -sub_pc_type lu
        -sub_pc_factor_mat_solver_type mumps
        -sub_mat_mumps_icntl_14 100
        -sub_ksp_type preonly
        -ksp_type fgmres
        -ksp_gmres_restart 220
        -ksp_rtol 1.e-9
        -ksp_atol 1.e-20
        -ksp_converged_reason

        -hard_pc_type bjacobi
        -hard_pc_bjacobi_blocks 4
        -hard_sub_pc_type lu
        -hard_sub_pc_factor_mat_solver_type mumps
        -hard_sub_mat_mumps_icntl_14 100
        -hard_sub_ksp_type preonly
        -hard_ksp_type fgmres
        -hard_ksp_gmres_restart 220
        -hard_ksp_rtol 1.e-9
        -hard_ksp_atol 1.e-20
        -hard_ksp_converged_reason

        -on_error_abort

   Please change the number of blocks (here 4) to match the number of plane in your 'C1input' file.

   We have another set of options with 'hard_" as the prefix for PETSC VERSION 3.8 or newer
   to isolate the hard solves (#5, #17) from easier ones for optimization purpose.

7. run job

7.1 interactive mode
  salloc -n 48 -t 00:30:00 -p sched_mit_psfc_r8
  export SLURM_CPU_BIND="cores"
  srun ... (see 7.2)

7.2 batch mode
   sample slurm script "batchjob.mit_gcc"

   #!/bin/bash
   #SBATCH -p sched_mit_psfc_r8
   #SBATCH -n 48
   #SBATCH -J m3dc1_regtest_adapt
   #SBATCH --time 0:30:00
   #SBATCH -o C1stdout

   export SLURM_CPU_BIND="cores"

   srun -n 32 -c 8 m3dc1_2d_complex -pc_factor_mat_solver_type mumps
   srun -n 32 -c 8 m3dc1_2d -pc_factor_mat_solver_type mumps
   srun -n 32 -c 8 m3dc1_3d -options_file options_bjacobi.type_mumps

8. regression tests
   cd unstructured
   export M3DC1_MPIRUN=srun M3DC1_VERSION=local M3DC1_ARCH=mit_gcc
   make bin ARCH=$M3DC1_ARCH
   cd _mit_gcc/bin/; export PATH=`pwd`:$PATH
   cd ../../regtest/
   ./run $M3DC1_ARCH
   ./check $M3DC1_ARCH


