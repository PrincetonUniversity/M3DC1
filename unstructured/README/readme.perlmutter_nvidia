1. login: login from cori 
   ssh -P cori.nersc.gov -l [username]
   ssh perlmutter

   direct login
   ssh -P perlmutter-p1.nersc.gov -l [username]
   or
   ssh -P saul-p1.nersc.gov -l [username]

2. code
   git clone git@github.com:PrincetonUniversity/M3DC1.git 
   git clone -b [branch] git@github.com:PrincetonUniversity/M3DC1.git 
   git commit -m "perlmutter code porting"
   git push origin master

3. load modules and set environment
   module load PrgEnv-nvidia cudatoolkit craype-accel-nvidia80 gsl
   module unload darshan

   export PATH=$CRAY_CUDATOOLKIT_DIR/bin:$PATH
   export LD_LIBRARY_PATH=$CRAY_CUDATOOLKIT_DIR/lib64:$LD_LIBRARY_PATH

4. compile code

   real 2D: make OPT=1 ARCH=perlmutter-nvidia
   complex: make OPT=1 COM=1 ARCH=perlmutter-nvidia
   real 3D: make 3D=1 OPT=1 MAX_PTS=60 ARCH=perlmutter-nvidia

5. mesh utility
   see $SCOREC_UTIL_DIR in perlmutter-gnu.mk

6. options_bjacobi for 3D

        -pc_type bjacobi
        -pc_bjacobi_blocks 4
        -sub_pc_type lu
        -sub_pc_factor_mat_solver_type mumps
        -sub_ksp_type preonly
        -ksp_type fgmres
        -ksp_gmres_restart 220
        -ksp_rtol 1.e-9
        -ksp_atol 1.e-20
        -ksp_converged_reason

        -hard_pc_type bjacobi
        -hard_pc_bjacobi_blocks 4
        -hard_sub_pc_type lu
        -hard_sub_pc_factor_mat_solver_type mumps
        -hard_sub_ksp_type preonly
        -hard_ksp_type fgmres
        -hard_ksp_gmres_restart 220
        -hard_ksp_rtol 1.e-9
        -hard_ksp_atol 1.e-20
        -hard_ksp_converged_reason

        -on_error_abort

   Please change the number of blocks (here 4) to match the number of plane in your 'C1input' file.

   We have another set of options with 'hard_" as the prefix for PETSC VERSION 3.8 or newer
   to isolate the hard solves (#5, #17) from easier ones for optimization purpose.

7. run job

7.1 interactive mode

  salloc -A mp288 -C gpu -n N -t hh:mm:ss
  export SLURM_CPU_BIND="cores"

  srun ... (see 7.2)

7.2 batch mode
   sample slurm script "batchjob.perlmutter_gpu"

   #!/bin/bash
   #SBATCH -A mp288 
   #SBATCH -p debug
   #SBATCH -C gpu
   #SBATCH -n 16 
   #SBATCH -J m3dc1_regtest_adapt
   #SBATCH -t 0:10:00
   #SBATCH -o C1stdout
   #SBATCH --gpus-per-node=4

   export SLURM_CPU_BIND="cores"
   echo $SLURM_CPUS_ON_NODE

   export MPICH_GPU_SUPPORT_ENABLED=1
   export CRAY_ACCEL_TARGET=nvidia80

   srun -n 16 -c 4 m3dc1_2d_complex -pc_factor_mat_solver_type mumps
   srun -n 16 -c 4 m3dc1_2d -pc_factor_mat_solver_type mumps
   srun -n 16 -c 4 m3dc1_3d -options_file options_bjacobi.type_mumps

8. regression tests
