

******************************************************************************
 NOTE: please add the following runtime options at your srun line:

 -sub_mat_superlu_dist_rowperm norowperm (when using superlu_dist for 3D run)
 -sub_mat_mumps_icntl_14 100 (when using mumps for 3D run)
******************************************************************************

1. login 
	ssh your_id@stellar.princeton.edu

	Nodes have quad sockets with 24 cores/socket  per node.  There is
	8GB/core available so a MUCH larger memory footprint.  The back end
	network is 100Gb Infiniband, HDR100.

2. code
   git clone https://your_id:password@github.com/PrincetonUniversity/M3DC1.git

3. load modules

   for release version modules:
       module use /home/nferraro/modules

   for development modules:
       setenv M3DC1_CODE_DIR $HOME/src/M3DC1  [for example]
       module use $M3DC1_CODE_DIR/unstructured/modules/stellar
       module load m3dc1/devel


   if you want to forgo the above modules and load the modules necessary
   for the latest code version individually:   

  module load cmake/3.19.7 intel/2021.1.2 intel-mpi/intel/2021.3.1
  module load fftw/intel-2021.1/intel-mpi/3.3.9
  module load hdf5/intel-2021.1/intel-mpi/1.10.6 gsl/2.6

4. compile code

   2D real: make OPT=1 RL=1 MAX_PTS=25 ARCH=stellar
   2D complex: make OPT=1 COM=1 MAX_PTS=25 ARCH=stellar
           - add PAR=1 to run PIC
   3D real: make 3D=1 OPT=1 MAX_PTS=60 ARCH=stellar
   after compiling, run "make bin"

5. mesh utility
5.1 mesh generation

  /projects/M3DC1/scorec/intel2021.1.2-intelmpi2021.3.1/17.0-220903/bin/

  See readme files or User's Guide (.pdf) for the details
 
5.2 mesh partition and collapse
  
  /projects/M3DC1/scorec/intel2021.1.2-intelmpi2021.3.1/petsc3.15.5/bin

6. run a job 

6.1 interactive mode
  
  $salloc -n N -t HH:MM:SS 
  $module load module load intel/2021.1 intel-mpi/intel/2021.3.1
  (complex) $srun -n N ./m3dc1_2d_complex -pc_factor_mat_solver_package mumps
  (2D real) $srun -n N  ./m3dc1_2d
  (3D real) $srun -n N ./m3dc1_3d -ipetsc -options_file options_bjacobi
  $exit

6.2 batch mode

  sample job_script

	#!/bin/bash
	#SBATCH -A pppl
	#SBATCH --nodes=16
	#SBATCH --ntasks-per-node=16
	#SBATCH -J JOB
	#SBATCH --time=00:30:00
        #2D complex
            srun -n 8 ./m3dc1_2d_complex -pc_factor_mat_solver_package mumps
        #2D real with petsc
            srun -n 8 ./m3dc1_2d
        #3D real with petsc
            srun -n 256 ./m3dc1_3d -ipetsc -options_file options_bjacobi


  submit job
	sbatch job_script

  list all curent jobs for a user
	squeue -u your_id

  delete a job
   	scancel job_id

6.3 regression test

	setenv M3DC1_MPIRUN srun
	setenv M3DC1_VERSION local
	setenv M3DC1_ARCH stellar (openmpi) OR  setenv M3DC1_ARCH stellar-intelmpi-sg (intelmpi)
	make bin ARCH=$M3DC1_ARCH
	cd _traverse/bin/
	setenv PATH `pwd`:$PATH
	cd ../../regtest/
	./run $M3DC1_ARCH
	./check $M3DC1_ARCH
