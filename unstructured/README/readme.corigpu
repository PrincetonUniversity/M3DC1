Mon Jul  8 09:39:44 PDT 2019

1. login from cori 
	salloc -C gpu -N 1 -n 20 -G  8 -A m1759 -t 03:59:59 --exclusive -q special

2. code

3. load modules and set environment

    module purge
    module load esslurm
    module load cuda/10.2.89 pgi/19.10 mvapich2/2.3.2
    module load python cmake git 
    module load gsl fftw/3.3.8 hdf5-parallel/1.10.5-pgi
    module load adios/1.9.0

4. compile code
   cori_gpu_pgi.mk

   make OPT=1 RL=1 MAX_PTS=25 ARCH=cori_gpu_pgi
   make OPT=1 COM=1 MAX_PTS=25 ARCH=cori_gpu_pgi
   make 3D=1 OPT=1 MAX_PTS=60 ARCH=cori_gpu_pgi

5. mesh utility
   unavailable

6. run jobscript:

   6.1 options_bjacobi *****

        -pc_type bjacobi
        -pc_bjacobi_blocks 4
        -sub_pc_type lu
        -sub_pc_factor_mat_solver_type superlu_dist
        -sub_ksp_type preonly
        -ksp_type fgmres
        -ksp_gmres_restart 220
        -ksp_rtol 1.e-9
        -ksp_atol 1.e-20
        -ksp_converged_reason

        -hard_pc_type bjacobi
        -hard_pc_bjacobi_blocks 4
        -hard_sub_pc_type lu
        -hard_sub_pc_factor_mat_solver_type superlu_dist
        -hard_sub_ksp_type preonly
        -hard_ksp_type fgmres
        -hard_ksp_gmres_restart 220
        -hard_ksp_rtol 1.e-9
        -hard_ksp_atol 1.e-20
        -hard_ksp_converged_reason

        -on_error_abort

   Please change the number of blocks (here 4) to match the number of plane in your 'C1input' file.

   We have another set of options with 'hard_" as the prefix for PETSC VERSION 3.8 or newer
   to isolate the hard solves (#5, #17) from easier ones for optimization purpose.

7. submit job: sample slurm script
   #!/bin/bash
   #SBATCH --account=m1759
   #SBATCH -J XX
   #SBATCH --nodes=2
   #SBATCH --tasks-per-node=16
   #SBATCH -C gpu
   #SBATCH -G 16
   #SBATCH -t 0:29:59
   #SBATCH -o C1stdout
   srun -n 32 m3dc1_2d_complex 


8. list all curent jobs for a user

9. delete a job

10. setup idl
