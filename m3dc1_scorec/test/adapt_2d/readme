+++++++++++++++++++++++++++++++++++++++++++
  Stand-alone test for 2D mesh adaptation
+++++++++++++++++++++++++++++++++++++++++++

[Stellar Intel MPI]
module purge
module load intel/2021.1.2
module load intel-mpi/intel/2021.3.1 cmake/3.19.7
module load fftw/intel-2021.1/intel-mpi/3.3.9
module load hdf5/intel-2021.1/intel-mpi/1.10.6

export MPIVER=intel2021.1.2-intelmpi2021.3.1
export PETSC_DIR=/projects/M3DC1/PETSC/petsc-3.15.5
export PETSC_ARCH=real-$MPIVER
export ZOLTAN_DIR=$PETSC_DIR/$PETSC_ARCH
export PUMI_DIR=YOUR_PUMI_BUILD or 
                /projects/M3DC1/scorec/$MPIVER/petsc3.15.5
export PREFIX=YOUR_M3DC1_SCOREC_BUILD or 
              /projects/M3DC1/scorec/$MPIVER/petsc3.15.5

Note that if you use default PUMI installed in /projects/M3DC1/scorec/$MPIVER/petsc3.15.5,
provide "-DOLDMA" in compilation flags to compile m3dc1_scorec and test programs

#real
mpiicpc -g main.cc -o adapt -DOLDMA -DM3DC1_PETSC -I/usr/include -I$I_MPI_ROOT/include -I$PREFIX/include  -I$PUMI_DIR/include -I$PETSC_DIR/include -I$PETSC_DIR/$PETSC_ARCH/include -L$PREFIX/lib -lm3dc1_scorec -Wl,--start-group,-rpath,$PUMI_DIR/lib -L$PUMI_DIR/lib -lpumi -lapf -lapf_zoltan -lgmi -llion -lma -lmds -lmth -lparma -lpcu -lph -lsam -lspr -lcrv -Wl,--end-group -L$ZOLTAN_DIR/lib -lzoltan -L$PETSC_DIR/$PETSC_ARCH/lib -Wl,-rpath,$PETSC_DIR/$PETSC_ARCH/lib -L$PETSC_DIR/$PETSC_ARCH/lib -lpetsc -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lsuperlu -lsuperlu_dist -lscalapack -lflapack -lfblas -lzoltan -lX11 -lparmetis -lmetis -lz -lmpifort -lmpi -lrt -lpthread -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl -L$FFTW3DIR/lib -lfftw3 -lfftw3f_mpi -lfftw3l_mpi -lfftw3_mpi -L$HDF5DIR/lib64 -lhdf5 -lhdf5_fortran -lhdf5_hl -lhdf5hl_fortran

For a complex version, 
  - modify PETSC_ARCH
  - add "-DPETSC_USE_COMPLEX" to compilation flags 
  - replace m3dc1_scorec library with m3dc1_scorec_complex

salloc -n P -t 00:30:00
srun -n P ./adapt input_args
exit


[Stellar OpenMPI]
module load intel/2021.1.2 openmpi/intel-2021.1/4.1.0 
module load fftw/intel-2021.1/openmpi-4.1.0/3.3.9 hdf5/intel-2021.1/openmpi-4.1.0/1.10.6

export MPIVER=intel2021.1.2-openmpi4.1.0 
export PETSC_DIR=/projects/M3DC1/PETSC/petsc-3.15.5
export PETSC_ARCH=real-$MPIVER
export ZOLTAN_DIR=$PETSC_DIR/$PETSC_ARCH
export PUMI_DIR=YOUR_PUMI_BUILD or 
                /projects/M3DC1/scorec/$MPIVER/petsc3.15.5
export PREFIX=YOUR_M3DC1_SCOREC_BUILD or 
                /projects/M3DC1/scorec/$MPIVER/petsc3.15.5

Note that if you use default PUMI installed in /projects/M3DC1/scorec/$MPIVER/petsc3.15.5,
provide "-DOLDMA" in compilation flags to compile m3dc1_scorec and test programs

/usr/local/openmpi/4.1.0/intel20211/bin/mpicxx -g main.cc -o adapt -DOLDMA -DM3DC1_PETSC -I/usr/include -I$MPIHOME/include -I$PREFIX/include  -I$PUMI_DIR/include -I$PETSC_DIR/include -I$PETSC_DIR/$PETSC_ARCH/include -L$PREFIX/lib -lm3dc1_scorec -Wl,--start-group,-rpath,$PUMI_DIR/lib -L$PUMI_DIR/lib -lpumi -lapf -lapf_zoltan -lgmi -llion -lma -lmds -lmth -lparma -lpcu -lph -lsam -lspr -lcrv -Wl,--end-group -L$ZOLTAN_DIR/lib -lzoltan -L$PETSC_DIR/$PETSC_ARCH/lib -Wl,-rpath,$PETSC_DIR/$PETSC_ARCH/lib -L$PETSC_DIR/$PETSC_ARCH/lib -Wl,-rpath,/usr/local/fftw/intel-2021.1/openmpi-4.1.0/3.3.9/lib -L/usr/local/fftw/intel-2021.1/openmpi-4.1.0/3.3.9/lib -Wl,-rpath,/usr/local/hdf5/intel-2021.1/openmpi-4.1.0/1.10.6/lib64 -L/usr/local/hdf5/intel-2021.1/openmpi-4.1.0/1.10.6/lib64 -L/usr/local/openmpi/4.1.0/intel20211/lib64 -L/usr/local/fftw/intel-2021.1/openmpi-4.1.0/3.3.9/lib64 -L/opt/intel/oneapi/mkl/2021.1.1/lib/intel64 -L/opt/intel/oneapi/tbb/2021.1.1/lib/intel64/gcc4.8 -L/opt/intel/oneapi/compiler/2021.1.2/linux/lib -L/opt/intel/oneapi/compiler/2021.1.2/linux/compiler/lib/intel64_lin -L/usr/lib/gcc/x86_64-redhat-linux/8 -Wl,-rpath,/usr/local/openmpi/4.1.0/intel20211/lib64 -lpetsc -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu -lsuperlu_dist -lfftw3_mpi -lfftw3 -lflapack -lfblas -lzoltan -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lifport -lifcoremt -limf -lsvml -lm -lipgo -lirc -lpthread -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl


For a complex version, 
  - modify PETSC_ARCH 
  - add "-DPETSC_USE_COMPLEX" to compilation flags  
  - replace m3dc1_scorec library with m3dc1_scorec_complex

mpirun -n P ./adapt input_args [SLURM NOT NEEDED]

[SCOREC RHEL6]
PETSC_DIR=/lore/seol/petsc-3.7.6
PETSC_ARCH=real-openmpi1.6.5
PARMETIS_DIR=$PETSC_DIR/$PETSC_ARCH
PUMI_DIR=/lore/seol/openmpi1.6.5-petsc3.7.6-install
ZOLTAN_DIR=$PUMI_DIR

#real
/usr/local/openmpi/latest/bin/mpicxx -g main.cc -o adapt -DM3DC1_PETSC -I/usr/include -I/usr/local/openmpi/latest/include -I$PREFIX/include -I$PUMI_DIR/include -I$PETSC_DIR/include -I$PETSC_DIR/$PETSC_ARCH/include -L$PREFIX/lib -lm3dc1_scorec -Wl,--start-group,-rpath,$PUMI_DIR/lib -L$PUMI_DIR/lib -lpumi -lapf -lapf_zoltan -lgmi -llion -lma -lmds -lmth -lparma -lpcu -lph -lsam -lspr -lcrv -Wl,--end-group -L$ZOLTAN_HOME/lib -lzoltan -L$PETSC_DIR/$PETSC_ARCH/lib -Wl,-rpath,$PETSC_DIR/$PETSC_ARCH/lib -L/usr/local/openmpi/1.6.5-ib/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.4.5 -L/usr/lib/x86_64-linux-gnu -lpetsc -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lHYPRE -lmpi_cxx -lstdc++ -lfftw3_mpi -lfftw3 -lflapack -lfblas -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lz -lgfortran -lmpi_f90 -lmpi_f77 -lmpi_cxx -lstdc++ -L/usr/local/openmpi/1.6.5-ib/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.4.5 -L/usr/lib/x86_64-linux-gnu -lmpi_cxx -lstdc++ -L/usr/local/openmpi/1.6.5-ib/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.4.5 -L/usr/lib/gcc/x86_64-linux-gnu/4.4.5 -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lm -lnuma -lrt -lnsl -lutil -lgcc_s -lpthread -ldl

[SCOREC RHEL7]
PETSC_DIR=/lore/seol/petsc-3.13.5
PETSC_ARCH=real-gcc4.8.5-v5m6xwi-mpich3.2.1-geowaxe
ZOLTAN_DIR=$PETSC_DIR/$PETSC_ARCH
PUMI_DIR=/lore/seol/rhel7-gcc4.8.5-v5m6xwi-mpich3.2.1-geowaxe
PREFIX=$PUMI_DIR

#real
mpicxx -g main.cc -o adapt -DM3DC1_PETSC -I/usr/include -I$PREFIX/include -I$PUMI_DIR/include -I$PETSC_DIR/include -I$PETSC_DIR/$PETSC_ARCH/include -L$PREFIX/lib -lm3dc1_scorec -Wl,--start-group,-rpath,$PUMI_DIR/lib -L$PUMI_DIR/lib -lpumi -lapf -lapf_zoltan -lgmi -llion -lma -lmds -lmth -lparma -lpcu -lph -lsam -lspr -lcrv -Wl,--end-group -L$ZOLTAN_DIR/lib -lzoltan -L$PETSC_DIR/$PETSC_ARCH/lib -Wl,-rpath,$PETSC_DIR/$PETSC_ARCH/lib -L$PETSC_DIR/$PETSC_ARCH/lib -L/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-4.8.5/mpich-3.2.1-geowaxeufjfz7vgpszmfrm4l6rp4rnxo/lib -Wl,-rpath,/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-rhel7_4.8.5/gcc-4.8.5-v5m6xwipewvkufflu3zvxs2x6jv4libr/lib:/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-rhel7_4.8.5/gcc-4.8.5-v5m6xwipewvkufflu3zvxs2x6jv4libr/lib64 -L/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-rhel7_4.8.5/gcc-4.8.5-v5m6xwipewvkufflu3zvxs2x6jv4libr/lib64 -L/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-rhel7_4.8.5/gcc-4.8.5-v5m6xwipewvkufflu3zvxs2x6jv4libr/lib/gcc/x86_64-unknown-linux-gnu/4.8.5 -L/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-4.8.5/hdf5-1.10.4-wousvnks7b7aplenali7jg5bsh243hyd/lib -L/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-4.8.5/zlib-1.2.11-vhzh5cfaki5lx5sjuth5iuojq5azdkbd/lib -L/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-rhel7_4.8.5/gcc-4.8.5-v5m6xwipewvkufflu3zvxs2x6jv4libr/lib -Wl,-rpath,/opt/scorec/spack/install/linux-rhel7-x86_64/gcc-4.8.5/mpich-3.2.1-geowaxeufjfz7vgpszmfrm4l6rp4rnxo/lib -lpetsc -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -lsuperlu -lsuperlu_dist -lfftw3_mpi -lfftw3 -lflapack -lfblas -lzoltan -lpthread -lX11 -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl

#complex
Add -DPETSC_USE_COMPLEX and replace -lm3dc1_scorec with -lm3dc1_scorec_complex


[Usage]
mpirun -np P ./adapt model mesh #planes model/mesh_load_option
#planes: if not provided, the default is -1
         -1 for PUMI model and mesh (.dmg and .smb)
         1 for 2D with M3DC1 model and mesh (.txt and .smb)
         n>1 for 3D with M3DC1 model and mesh (.txt and .smb)
 
load_option: if not provided, the default is 0 
             0 for loading M3DC1 model (.txt) and 2D mesh file for both 2D and 3D run
             1 for loading M3DC1 model (.txt) and 3D mesh file for 3D run (no 3D mesh construction)

[Examples]
(1) load .dmg model and 2D mesh
mpirun -np 1 ./adapt /lore/seol/meshes/xgc/g096333.03337_pumiModel.dmg /lore/seol/meshes/xgc/g096333.03337_pumiMesh.smb 

(2) load .dmg model and 3D mesh
mpirun -np 4 ./adapt /lore/seol/meshes/torus/torus.dmg /lore/seol/meshes/torus/4imb/torus.smb

(3) load .txt model and 2D mesh then construct 3D mesh
mpirun -np 4  ./adapt tilt.txt tilt.smb 2
mpirun -np 8 ./adapt tilt.txt tilt.smb 4
// FIXME: crash in apf::reorderMdsMesh after adaptation
mpirun -np 64 ./adapt ../nate-32part/diiid0.05.txt ../nate-32part/mesh.smb 2 
mpirun -np 16  ./adapt /lore/seol/tests/pellet_example/analytic.txt /lore/seol/tests/pellet_example/part.smb 1

(4) load .txt model and distributed 3D mesh (no 3D mesh extrusion)
// FIXME: crash in the beginning of adaptation
mpirun -np 64 ./adapt /lore/seol/tests/adapt/3d-adapt-64p/diiid-0.02-2.5-4.0.txt /lore/seol/tests/adapt/3d-adapt-64p/part_64p.smb 4 1


