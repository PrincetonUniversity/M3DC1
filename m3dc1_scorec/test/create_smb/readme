============================================
create_smb: create mesh 

 * input: ./create_smb in-model relative_mesh_size
   • in-model is an ascii file with 5 doubles: a_param, b_param, c_param, d_param, e_param 
   • relative_mesh_size is desired mesh edge length divided by the longest edge of the bounding box of the model
   • the file "seed0.smb" should exist in the work dir
 * output: mesh PUMI and VTK format
   •  model_file0.smb: SCOREC PUMI-readable mesh file in binary format
   •  model_file0.vku and modelfile.pvtk: Kitware Paraview-readable file for visualization purpose
 * to compile: copy create_smb.cc into core/test/fusion.cc and do "make fusion"

============================================
split_smb: split P-part mesh into N parts

 * input: mpirun -n N ./split_smb in-model.dmg in-mesh.smb out-mesh.smb N/P
   • to generate in-model.dmg, run mkmodel available in core/test/mkmodel.cc
        mpirun -n P ./mkmodel in-mesh.smb model.dmg
 * output: N-part mesh (.smb)
 * to compile: copy split_smb.cc into core/test/zsplit.cc and do "make zsplit"

============================================
show_meshcount: given P-part mesh and #planes (N/P), display mesh entity count per plane and per part

 * input: mpirun -n N ./show_meshcount in-model in-mesh.smb #plane
   • in-model is an M3D-C1 readable analytic model file
   • #plane shall be N/P (N: #proc, P: #parts in in-mesh)
 * to compile on SCOREC:
   /usr/local/openmpi/latest/bin/mpicc show_meshcount.cc -o show_meshcount  -DDEBUG -I/usr/local/openmpi/latest/include  -I/fasttmp/seol/petsc-3.5.4-real/openmpi1.6.5/include -I/fasttmp/seol/petsc-3.5.4-real/include -I/fasttmp/seol/openmpi-gcc4.7.0-install/include -Wl,--start-group,-rpath,/fasttmp/seol/openmpi-gcc4.7.0-install/lib -L/fasttmp/seol/openmpi-gcc4.7.0-install/lib -lcrv -ldsp -lph -lsize -lsam -lspr -lma -lapf_zoltan -lparma -lmds -lapf -llion -lmth -lgmi -lpcu -lm3dc1_scorec -Wl,--end-group  -L/fasttmp/seol/openmpi-gcc4.7.0-install/lib -lzoltan -L/fasttmp/seol/petsc-3.5.4-real/openmpi1.6.5/lib  -lpetsc -Wl,-rpath,/fasttmp/seol/petsc-3.5.4-real/openmpi1.6.5/lib -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_4.3 -lsuperlu_dist_3.3 -lHYPRE -lparmetis -lmetis -L/usr/local/openmpi/latest/lib -L/usr/local/gcc/4.7.0/lib/gcc/x86_64-unknown-linux-gnu/4.7.0 -L/usr/local/gcc/4.7.0/lib64 -L/usr/local/gcc/4.7.0/lib -Wl,-rpath,/usr/local/openmpi/latest/lib -lmpi_cxx -lstdc++ -llapack -lblas -lparmetis -lmetis -lpthread -lssl -lcrypto -lnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lz -lmpi_f90 -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/local/openmpi/latest/lib -L/usr/local/gcc/4.7.0/lib/gcc/x86_64-unknown-linux-gnu/4.7.0 -L/usr/local/gcc/4.7.0/lib64 -L/usr/local/gcc/4.7.0/lib -ldl -Wl,-rpath,/usr/local/openmpi/latest/lib -lmpi -lrt -lpthread -lgcc_s -ldl

 * to run 2D on SCOREC: /usr/local/openmpi/latest/bin/mpirun -np 4 ./show_meshcount ../AnalyticModel ../1K-4part/part.smb 1
 * to run 3D on SCOREC: /usr/local/openmpi/latest/bin/mpirun -np 12 ./show_meshcount ../AnalyticModel ../1K-4part/part.smb 3
